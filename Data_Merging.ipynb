{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexagalicia/codepath-prework/blob/main/Data_Merging.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries"
      ],
      "metadata": {
        "id": "D84dfOzMqspx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "OLMNpGq5qsT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Format DateTime objects"
      ],
      "metadata": {
        "id": "QBOxGwuJK1bl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string_date_format = \"%Y-%m-%d\""
      ],
      "metadata": {
        "id": "4gLybiPwK0c2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Datasets"
      ],
      "metadata": {
        "id": "i9FIpHwEq0nI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fsd = pd.read_csv('franchisee_service_data.csv')\n",
        "ad = pd.read_csv('cleaned_admin_data.csv')"
      ],
      "metadata": {
        "id": "wNS4sq9Bq2ni",
        "outputId": "6ef7525c-206f-48b0-9f7a-a1eb1e9af625",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'franchisee_service_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2466178126.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfsd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'franchisee_service_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cleaned_admin_data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'franchisee_service_data.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardize Naming Convetion of clean franchisee dataset"
      ],
      "metadata": {
        "id": "88pWBEhFMIlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this will standardize everything and fixes other typos\n",
        "\n",
        "group_mapping = {\n",
        "    # Standardize YESCO capitalization\n",
        "    'Yesco Ontario Southwest': 'YESCO Ontario Southwest',\n",
        "    'Yesco Calgary': 'YESCO Calgary',\n",
        "    'Yesco Chicago': 'YESCO Chicago',\n",
        "    'Yesco Cleveland': 'YESCO Cleveland',\n",
        "    'Yesco Colorado Springs': 'YESCO Colorado Springs',\n",
        "    'Yesco Concord - Nashua': 'YESCO Concord - Nashua',\n",
        "    'Yesco Denver': 'YESCO Denver',\n",
        "    'Yesco Fort Lauderdale': 'YESCO Fort Lauderdale',\n",
        "    'Yesco Fort Myers': 'YESCO Fort Myers',\n",
        "    'Yesco Hampton Roads': 'YESCO Hampton Roads',\n",
        "    'Yesco Louisville': 'YESCO Louisville',\n",
        "    'Yesco Mid Atlantic': 'YESCO Mid Atlantic',\n",
        "    'Yesco Mid-Atlantic': 'YESCO Mid Atlantic',  # Consolidate the hyphen variant\n",
        "    'Yesco Nashville / Florida Panhandle': 'YESCO Nashville / Florida Panhandle',\n",
        "    'Yesco New Jersey Central & Coast': 'YESCO New Jersey Central & Coast',\n",
        "    'Yesco New Jersey Central & South': 'YESCO New Jersey Central & South',\n",
        "    'Yesco North Carolina Central': 'YESCO North Carolina Central',\n",
        "    'Yesco North Carolina East': 'YESCO North Carolina East',\n",
        "    'Yesco Oklahoma / New England South': 'YESCO Oklahoma / New England South',\n",
        "    'Yesco Orlando North': 'YESCO Orlando North',\n",
        "    'Yesco Wilchita': 'YESCO Wichita',  # Fix typo\n",
        "    'Yesco Wisconsin Central': 'YESCO Wisconsin Central',\n",
        "    'Yesco Whitby': 'YESCO Whitby',\n",
        "\n",
        "    # Keep these as-is (already correct format)\n",
        "    'Franchising Team': 'Franchising Team',\n",
        "    'Px Improvement': 'PX Improvement',\n",
        "    'Wiin': 'WIIN',\n",
        "    'Yesco8 Cincinnati - Dayton': 'YESCO Cincinnati - Dayton',  # Remove the \"8\"\n",
        "    'Hampton Roads': 'YESCO Hampton Roads',  # Add YESCO prefix\n",
        "    'Yesco Hampton Roads / Richmond': 'YESCO Hampton Roads / Richmond',\n",
        "    'Yesco Hampton Roads South / Dallas South': 'YESCO Hampton Roads South / Dallas South',\n",
        "    'Yesco Myrtle Beach': 'YESCO Myrtle Beach',\n",
        "}\n",
        "\n",
        "# Apply the mapping\n",
        "fsd['Tenant Name'] = fsd['Tenant Name'].replace(group_mapping)\n",
        "\n",
        "print(\"Standardized Group names\")\n",
        "print(\"\\nUnique groups after cleaning:\")\n",
        "# Filter out NaN values before sorting\n",
        "unique_groups_cleaned = [group for group in fsd['Tenant Name'].unique() if isinstance(group, str)]\n",
        "print(sorted(unique_groups_cleaned))\n",
        "print(f\"\\nTotal unique groups: {len(unique_groups_cleaned)}\")"
      ],
      "metadata": {
        "id": "rrd2zpDOMSjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Further Clean Admin Data to be in the format:\n",
        "\n",
        "```\n",
        "Orginization | Date | Stage #\n",
        "```\n",
        "Where Stage # is a numerical representation of how far through the program they have made it\n",
        "\n"
      ],
      "metadata": {
        "id": "aAUpmvWCSKhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the relevant columns for stage calculation\n",
        "stage_columns = ad.columns[ad.columns.get_loc('Setup Vision'):]\n",
        "\n",
        "# Function to determine the stage number\n",
        "def get_stage(row):\n",
        "    stage = 0\n",
        "    for i, col in enumerate(stage_columns):\n",
        "        if row[col] == 2:\n",
        "            stage = i + 1  # Stages are 1-indexed based on the column order\n",
        "    return stage\n",
        "\n",
        "# Apply the function to create the 'Stage #' column\n",
        "ad['Stage'] = ad.apply(get_stage, axis=1)\n",
        "\n",
        "# Select only the required columns\n",
        "cleaned_ad = ad[['Group', 'Date', 'Stage']]\n",
        "\n",
        "# Filter out rows where Organization name does not start with \"YESCO\"\n",
        "cleaned_ad = cleaned_ad[cleaned_ad['Group'].str.startswith('YESCO', na=False)].copy()\n",
        "cleaned_ad = cleaned_ad.rename(columns={\"Group\": \"Organization\"})\n",
        "\n",
        "# Display the cleaned DataFrame\n",
        "display(cleaned_ad.head())"
      ],
      "metadata": {
        "id": "GcfeEn6OSeMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5f9ff90"
      },
      "source": [
        "# Sort the cleaned_ad DataFrame by 'Organization' and 'Date'\n",
        "sorted_cleaned_ad = cleaned_ad.sort_values(by=['Organization', 'Date'])\n",
        "\n",
        "# Calculate the cumulative maximum stage for each organization\n",
        "sorted_cleaned_ad['Stage'] = sorted_cleaned_ad.groupby('Organization')['Stage'].cummax()\n",
        "\n",
        "# Display the sorted DataFrame with cumulative maximum stage\n",
        "ad = sorted_cleaned_ad\n",
        "display(ad.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new data frame to be in the following format to encapsulate all important information for analysis\n",
        "\n",
        "\n",
        "```\n",
        "ID | Name | Stage @ Point in Time | Date | Subtotal\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "MZA2MjU6qiLT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fsd['Date Created'] = pd.to_datetime(fsd['Date Created'])\n",
        "ad['Date'] = pd.to_datetime(ad['Date'])\n",
        "\n",
        "fsd = fsd.rename(columns={\"Tenant ID\": \"ID\", \"Tenant Name\": \"Organization\", \"Date Created\": \"Date\", \"Subtotal\": \"Sale\"})\n",
        "fsd = fsd.drop('Servizio ID', axis=1)\n",
        "\n",
        "display(fsd)"
      ],
      "metadata": {
        "id": "bFJPYUDRqrZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge fsd and ad on 'Organization'\n",
        "# We use a left merge on fsd to keep all sales data\n",
        "# We merge based on the date such that we get the stage at or before the sale date\n",
        "df = pd.merge_asof(fsd.sort_values('Date'), ad.sort_values('Date'), on='Date', by='Organization', direction='backward')\n",
        "\n",
        "# Display the resulting dataframe\n",
        "display(df)"
      ],
      "metadata": {
        "id": "MjOJtLPCYp5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visual Analysis of the Data"
      ],
      "metadata": {
        "id": "zKqlezf9qyaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "org_name = 'YESCO Louisville'\n",
        "org_df = df[df['Organization'] == org_name].copy()\n",
        "\n",
        "# Find the date when stage 1 was reached\n",
        "stage_1_date = ad[ad['Organization'] == org_name][ad['Stage'] >= 1]['Date'].min()\n",
        "\n",
        "# Filter data before reaching stage 1\n",
        "org_df_before_stage_1 = org_df[org_df['Date'] < stage_1_date].copy()\n",
        "\n",
        "# Filter data after reaching stage 1\n",
        "org_df_after_stage_1 = org_df[org_df['Date'] >= stage_1_date].copy()\n",
        "\n",
        "# Calculate weekly revenue before stage 1\n",
        "org_df_before_stage_1['Week'] = org_df_before_stage_1['Date'].dt.to_period('W')\n",
        "weekly_revenue_before = org_df_before_stage_1.groupby('Week')['Sale'].sum().reset_index()\n",
        "weekly_revenue_before['Week'] = weekly_revenue_before['Week'].dt.to_timestamp()\n",
        "\n",
        "# Calculate weekly revenue after stage 1\n",
        "org_df_after_stage_1['Week'] = org_df_after_stage_1['Date'].dt.to_period('W')\n",
        "weekly_revenue_after = org_df_after_stage_1.groupby('Week')['Sale'].sum().reset_index()\n",
        "weekly_revenue_after['Week'] = weekly_revenue_after['Week'].dt.to_timestamp()\n",
        "\n",
        "# Create the first plot (before stage 1)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(weekly_revenue_before['Week'], weekly_revenue_before['Sale'], marker='o', linestyle='-', label='Weekly Revenue Before Stage 1')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Weekly Revenue')\n",
        "plt.title(f'Weekly Revenue Before Stage 1 for {org_name}')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Create the second plot (after stage 1)\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(weekly_revenue_after['Week'], weekly_revenue_after['Sale'], marker='o', linestyle='-', label='Weekly Revenue After Stage 1')\n",
        "\n",
        "# Mark stage changes on the second plot\n",
        "stage_changes_org = ad[(ad['Organization'] == org_name) & (ad['Stage'].notna())].copy()\n",
        "stage_change_dates = stage_changes_org['Date'].tolist()\n",
        "stage_change_stages = stage_changes_org['Stage'].tolist()\n",
        "\n",
        "legend_handles = []\n",
        "for date, stage in zip(stage_change_dates, stage_change_stages):\n",
        "    line = plt.axvline(date, color='r', linestyle='--', lw=1, label=f'Stage {int(stage)} Change')\n",
        "    legend_handles.append(line)\n",
        "\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Weekly Revenue')\n",
        "plt.title(f'Weekly Revenue and Stage Changes After Stage 1 for {org_name}')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OlHbw-bzq0nL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7de4341f"
      },
      "source": [
        "\n",
        "Display graphs from the top 10% of organizations who have progressed incrementally through changes. Creates a polynomial line of best fit and displays where the greatest increase in revenue was\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6af1e56"
      },
      "source": [
        "# Filter the df DataFrame to include only rows where the 'Stage' column is not NaN.\n",
        "stage_changes = df[df['Stage'].notna()].copy()\n",
        "\n",
        "# Group stage_changes by 'Organization' and count the number of stage changes.\n",
        "stage_change_counts = stage_changes.groupby('Organization').size().reset_index(name='stage_change_count')\n",
        "\n",
        "# Calculate the number of organizations corresponding to the top 10%.\n",
        "num_unique_organizations = df['Organization'].nunique()\n",
        "top_10_percent_count = int(num_unique_organizations * 0.1)\n",
        "\n",
        "# Sort the grouped DataFrame by 'stage_change_count' in descending order.\n",
        "sorted_stage_change_counts = stage_change_counts.sort_values(by='stage_change_count', ascending=False)\n",
        "\n",
        "# Select the top top_10_percent_count organizations.\n",
        "top_organizations_df = sorted_stage_change_counts.head(top_10_percent_count)\n",
        "\n",
        "# Store their names in a list called top_organizations.\n",
        "top_organizations = top_organizations_df['Organization'].tolist()\n",
        "\n",
        "print(f\"Number of unique organizations: {num_unique_organizations}\")\n",
        "print(f\"Number of organizations in the top 10%: {top_10_percent_count}\")\n",
        "print(\"\\nTop organizations by stage change count:\")\n",
        "print(top_organizations)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74a3defe"
      },
      "source": [
        "from scipy.stats import linregress\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for org_name in top_organizations:\n",
        "    print(f\"\\n--- Analyzing {org_name} ---\")\n",
        "\n",
        "    org_df = df[df['Organization'] == org_name].copy()\n",
        "\n",
        "    # Calculate weekly revenue for the entire period\n",
        "    if not org_df.empty:\n",
        "        org_df['Week'] = org_df['Date'].dt.to_period('W')\n",
        "        weekly_revenue = org_df.groupby('Week')['Sale'].sum().reset_index()\n",
        "        weekly_revenue['Week'] = weekly_revenue['Week'].dt.to_timestamp()\n",
        "\n",
        "        # Convert datetime to numerical representation (timestamp)\n",
        "        weekly_revenue['Week_numeric'] = weekly_revenue['Week'].apply(lambda x: x.timestamp())\n",
        "\n",
        "        # Fit a polynomial regression model (degree 3 as a starting point)\n",
        "        degree = 3\n",
        "        coeffs = np.polyfit(weekly_revenue['Week_numeric'], weekly_revenue['Sale'], degree)\n",
        "        poly_model = np.poly1d(coeffs)\n",
        "\n",
        "        # Generate predicted revenue values\n",
        "        weekly_revenue['Non_Linear_Fit'] = poly_model(weekly_revenue['Week_numeric'])\n",
        "\n",
        "        # Calculate the derivative of the polynomial to find the slope\n",
        "        poly_derivative = np.polyder(poly_model)\n",
        "\n",
        "        # Calculate the slope at each point\n",
        "        weekly_revenue['Slope'] = poly_derivative(weekly_revenue['Week_numeric'])\n",
        "\n",
        "        # Find the point with the largest slope\n",
        "        max_slope_row = weekly_revenue.loc[weekly_revenue['Slope'].idxmax()]\n",
        "\n",
        "        # Create the plot\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(weekly_revenue['Week'], weekly_revenue['Sale'], marker='o', linestyle='-', label='Weekly Revenue')\n",
        "        plt.plot(weekly_revenue['Week'], weekly_revenue['Non_Linear_Fit'], color='purple', linestyle='--', linewidth=2, label='Non-Linear Best Fit')\n",
        "\n",
        "        # Mark all stage changes on the plot\n",
        "        stage_changes_org = ad[(ad['Organization'] == org_name) & (ad['Stage'].notna())].copy()\n",
        "\n",
        "        # Get unique stage changes by date to avoid duplicate vertical lines at the same date\n",
        "        unique_stage_changes = stage_changes_org.drop_duplicates(subset=['Date', 'Stage']).sort_values('Date')\n",
        "\n",
        "        # Add a single legend entry for stage changes\n",
        "        if not unique_stage_changes.empty:\n",
        "            plt.axvline(unique_stage_changes.iloc[0]['Date'], color='r', linestyle='--', lw=1, label='Stage Change')\n",
        "            for index, row in unique_stage_changes.iterrows():\n",
        "                date = row['Date']\n",
        "                # Only plot the vertical line without adding to the legend here\n",
        "                plt.axvline(date, color='r', linestyle='--', lw=1)\n",
        "\n",
        "        # Mark the point of largest slope on the best fit line\n",
        "        plt.plot(max_slope_row['Week'], max_slope_row['Non_Linear_Fit'], marker='X', markersize=10, color='green', label=f'Max Slope Point (Slope: {max_slope_row[\"Slope\"]:.2f})')\n",
        "\n",
        "\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel('Weekly Revenue')\n",
        "        plt.title(f'Weekly Revenue, Non-Linear Fit, and Stage Changes for {org_name}')\n",
        "        plt.grid(True)\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"No data found for {org_name}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save as a CSV/Excel"
      ],
      "metadata": {
        "id": "EjTt1IerqQNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#if you need to download the df as excel\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# Export to Excel\n",
        "df.to_excel('cleaned_combined_data.xlsx', index=False)\n",
        "\n",
        "# Download the file\n",
        "files.download('cleaned_combined_data.xlsx')\n",
        "print(\"âœ“ Download started!\")"
      ],
      "metadata": {
        "id": "dR06BImEqRsP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}